{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"TF.iDF_NER_Embedding_Maker.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2NlnnUatWC58","colab_type":"text"},"source":["### Import Data"]},{"cell_type":"code","metadata":{"id":"jDO7R84GWA00","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j43LGvZcaOT0","colab_type":"code","colab":{}},"source":["a = []\n","while(1):\n","    a.append(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBYZFQ67aOGW","colab_type":"code","colab":{}},"source":["# tx = pd.read_pickle(path+\"GS_dataframe_0_to_49.pickle\")\n","# (tx.set_index(np.arange(1000000, 1000050, 1))).to_pickle(path+\"GS_dataframe_0_to_49.pickle\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5odBboCVuUC","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","path = \"/content/drive/My Drive/IR End Review/dataframe/\"\n","\n","data = pd.DataFrame()\n","\n","for i in os.listdir(path):\n","    df = pd.read_pickle(path+i)\n","    df['temp_col_len'] = df.full.str.len()\n","    print(i)\n","    data = pd.concat([data, df[df['temp_col_len']>1].drop(columns = ['temp_col_len'])], axis = 0, ignore_index=True)\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbE9L_1DdJgN","colab_type":"code","colab":{}},"source":["GS_index = (np.array(data.index))[-50:]\n","GS_index"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRK6mNXnWJg8","colab_type":"text"},"source":["Split the body of Text into Sentences"]},{"cell_type":"code","metadata":{"id":"d32IP1NTVuUK","colab_type":"code","colab":{}},"source":["# Sentence tokenization\n","# and tagging of document number t sentences\n","\n","temp = data.copy()\n","\n","set = temp.index\n","# set = np.arange(0, 51, 1)\n","# set = [0, 1, 2]\n","\n","# temp = data\n","\n","full_sent = pd.DataFrame()\n","summ_sent = pd.DataFrame()\n","\n","full_docs = []\n","summ_docs = []\n","GS_docs = []\n","\n","ori_sent = pd.DataFrame()\n","GS_sent = pd.DataFrame()\n","GS_ori_sent = pd.DataFrame()\n","\n","for i in set:\n","  if i not in GS_index:\n","    full = temp['full'].iloc[i]\n","    summary = temp['summary'].iloc[i]\n","    ori = temp['full_orignal'].iloc[i]\n","    \n","#     print(full)\n","#     break\n","    v = full.split(\"*\")    \n","    t1 = pd.DataFrame(v)\n","    \n","    v = summary.split('*')\n","    t2 = pd.DataFrame(v)\n","    \n","    v = ori.split('*')\n","    t5 = pd.DataFrame(v)\n","    \n","    t3 = np.linspace(i, i, num = len(t1))\n","    t4 = np.linspace(i, i, num = len(t2))\n","\n","    # print(t1)\n","    \n","    if(len(t1)==len(t5)):\n","        full_docs = np.append(full_docs, t3)\n","        summ_docs = np.append(summ_docs, t4)\n","        full_sent = pd.concat([full_sent, t1], ignore_index=True)\n","        summ_sent = pd.concat([summ_sent, t2], ignore_index=True)\n","        ori_sent = pd.concat([ori_sent, t5], ignore_index=True)\n","    else:\n","        print(\"Doc No. : \", i,\"has a problem\")\n","  \n","  else:\n","    full = temp['full'].iloc[i]\n","    v = full.split(\"*\")    \n","    t1 = pd.DataFrame(v)\n","\n","    ori = temp['full_orignal'].iloc[i]\n","    v = ori.split('*')\n","    t5 = pd.DataFrame(v)\n","\n","    t3 = np.linspace(i, i, num = len(t1))\n","\n","    if(len(t1)==len(t5)):\n","        GS_docs = np.append(GS_docs, t3)\n","        GS_sent = pd.concat([GS_sent, t1], ignore_index=True)\n","        GS_ori_sent = pd.concat([GS_ori_sent, t5], ignore_index=True)\n","    else:\n","      print(\"GS Doc No. : \", i-1060,\"has a problem\")\n","\n","print('summary sentence tokenization')\n","print(len(summ_docs))\n","print(len(summ_sent))\n","print(len(full_docs))\n","print(len(full_sent))\n","print(len(ori_sent))\n","\n","print(len(GS_docs))\n","print(len(GS_sent))\n","print(len(GS_ori_sent))\n","\n","# summ_sent_mod = pd.DataFrame(summ_sent, columns=['Summary_Sentences'])\n","summ_sent_mod = summ_sent.set_axis(['Sentences'], axis=1, inplace=False)\n","# summ_sent_mod\n","\n","full_sent_mod = full_sent.set_axis(['Sentences'], axis=1, inplace=False)\n","# full_sent_mod"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"11T2bo74VuUM","colab_type":"code","colab":{}},"source":["# combining summary and full-text sentences for tfidf vectorization of sentences\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","GS_sent_mod = GS_sent.set_axis(['Sentences'], axis=1, inplace=False)\n","\n","tmp1 = pd.DataFrame(summ_sent_mod)\n","tmp2 = pd.DataFrame(full_sent_mod)\n","tmp3 = pd.DataFrame(GS_sent_mod)\n","combined = pd.concat([tmp1, tmp2], axis = 0, ignore_index=True, sort = False)\n","combined = pd.concat([combined, tmp3], axis = 0, ignore_index=True, sort = False)\n","combined.columns = ['sents']\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","print(combined.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZ-ECbbmVuUP","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer as tuttu\n","from nltk.tokenize import RegexpTokenizer\n","import scipy\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n","tfidf_func = tuttu(ngram_range = (1, 1), tokenizer = token.tokenize, lowercase = True)\n","tfidf = tfidf_func.fit_transform(combined['sents'])\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","tfidf.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Vl7Yby-VuUR","colab_type":"code","colab":{}},"source":["# dimension reduction\n","from sklearn.decomposition import PCA, TruncatedSVD\n","\n","svd = TruncatedSVD(100)\n","a = svd.fit_transform(tfidf[:,:])\n","\n","del tfidf\n","\n","print(a.shape)\n","print(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gc9lElqvVuUU","colab_type":"code","colab":{}},"source":["# ## Function to reduce the DF size\n","# def reduce_mem_usage(df, verbose=True):\n","#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","#     start_mem = df.memory_usage().sum() / 1024**2    \n","#     for col in df.columns:\n","#         col_type = df[col].dtypes\n","#         if col_type in numerics:\n","#             c_min = df[col].min()\n","#             c_max = df[col].max()\n","#             if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","#                 df[col] = df[col].astype(np.float16)\n","#             elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","#                 df[col] = df[col].astype(np.float32)\n","#             else:\n","#                 df[col] = df[col].astype(np.float64)  \n","#         if(col%20==0):\n","#             print(\"column no : \", col, \" done!\")\n","#     end_mem = df.memory_usage().sum() / 1024**2\n","#     if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","#     return df\n","\n","# x2 = pd.DataFrame(a)\n","# x2 = reduce_mem_usage(x2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RARMZyihVuUW","colab_type":"code","colab":{}},"source":["x2 = pd.DataFrame(a)\n","print(x2.shape)\n","\n","summ_tfidf = x2.iloc[:len(summ_sent_mod)]\n","full_tfidf = x2.iloc[len(summ_sent_mod):len(summ_sent_mod)+len(full_sent_mod)]\n","GS_tfidf = x2.iloc[len(summ_sent_mod)+len(full_sent_mod):]\n","\n","print(len(summ_docs))\n","print(len(full_docs))\n","print(len(GS_docs))\n","\n","summ_tfidf['doc_id'] = summ_docs\n","full_tfidf['doc_id'] = full_docs\n","GS_tfidf['doc_id'] = full_docs\n","\n","full_tfidf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5oNrqAkVuUY","colab_type":"code","colab":{}},"source":["from sklearn import preprocessing\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","summ_tfidf_normalized = preprocessing.normalize(summ_tfidf.drop(['doc_id'], axis=1), norm='l2')\n","summ_tfidf_normalized = pd.DataFrame(summ_tfidf_normalized)\n","summ_tfidf_normalized.columns = summ_tfidf.drop(['doc_id'], axis=1).columns\n","summ_tfidf_normalized['doc_id'] = np.array(summ_tfidf.doc_id)\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","full_tfidf_normalized = preprocessing.normalize(full_tfidf.drop(['doc_id'], axis=1), norm='l2')\n","full_tfidf_normalized = pd.DataFrame(full_tfidf_normalized)\n","full_tfidf_normalized.columns = full_tfidf.drop(['doc_id'], axis=1).columns\n","full_tfidf_normalized['doc_id'] = np.array(full_tfidf.doc_id)\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","summ_tfidf_normalized"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dS_0hiY2VuUa","colab_type":"code","colab":{}},"source":["A = [[2,3],[5,4]]\n","preprocessing.normalize(A, norm='l2')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJsjifWUVuUc","colab_type":"code","colab":{}},"source":["cosine_vec = []\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","for i in set:\n","  tmp_full_sent = summ_tfidf_normalized[summ_tfidf_normalized.doc_id==i].drop(['doc_id'], axis=1)\n","  tmp_summ_sent = full_tfidf_normalized[full_tfidf_normalized.doc_id==i].drop(['doc_id'], axis=1)\n","\n","  M = tmp_summ_sent.dot(np.transpose(tmp_full_sent))\n","  cos_doc = (np.amax(M, axis = 1))\n","  cosine_vec = np.append(cosine_vec, cos_doc)\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","len(cosine_vec)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0abhtFqNWtji","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"_FWIABoPWtns","colab_type":"text"},"source":["Cosine distribution"]},{"cell_type":"code","metadata":{"id":"nS4MzSXJVuUe","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","# -----------------------------------------------------------------------------------------------------------------------------\n","\n","plt.hist(cosine_vec, bins=20)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEY-0c5mVuUh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFk2WTSSVuUj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}